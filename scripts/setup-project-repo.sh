#!/bin/bash

# StackKit í”„ë¡œì íŠ¸ ë ˆí¬ì§€í† ë¦¬ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸  
# ê¸°ì¡´ ì¤‘ì•™ Atlantis ì„œë²„ì™€ ì—°ë™í•˜ì—¬ StackKit ëª¨ë“ˆì„ ì‚¬ìš©í•˜ëŠ” í”„ë¡œì íŠ¸ ì„¤ì •

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_ROOT="$(pwd)"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m' 
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
PROJECT_NAME=""
PROJECT_TYPE=""
ATLANTIS_URL=""
ATLANTIS_S3_BUCKET=""
AWS_REGION="ap-northeast-2"
ENVIRONMENT="dev"
AUTO_SETUP=false

show_usage() {
    cat << EOF
ğŸš€ StackKit í”„ë¡œì íŠ¸ ë ˆí¬ì§€í† ë¦¬ ì„¤ì •

ê¸°ì¡´ ì¤‘ì•™ Atlantis ì„œë²„ì™€ ì—°ë™í•˜ì—¬ StackKit ëª¨ë“ˆì„ ì‚¬ìš©í•˜ëŠ” í”„ë¡œì íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

Usage: $0 [options]

í•„ìˆ˜ ì˜µì…˜:
    -p, --project-name NAME     í”„ë¡œì íŠ¸ ì´ë¦„ (ì˜ˆ: my-web-app)
    -u, --atlantis-url URL      ì¤‘ì•™ Atlantis ì„œë²„ URL
    -b, --s3-bucket BUCKET      Atlantis S3 ë²„í‚· ì´ë¦„

ì„ íƒ ì˜µì…˜:
    -t, --type TYPE             í”„ë¡œì íŠ¸ íƒ€ì… (web-app|api|serverless|custom)
    -e, --environment ENV       í™˜ê²½ ì´ë¦„ (ê¸°ë³¸ê°’: dev) 
    --region REGION             AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)
    --auto                      ëŒ€í™”í˜• ì…ë ¥ ì—†ì´ ìë™ ì‹¤í–‰
    -h, --help                  ë„ì›€ë§ í‘œì‹œ

ì˜ˆì‹œ:
    # ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë¡œì íŠ¸
    $0 -p my-web-app -t web-app -u http://atlantis.example.com -b my-org-atlantis-artifacts
    
    # API ì„œë²„ í”„ë¡œì íŠ¸
    $0 -p my-api -t api -u http://atlantis.example.com -b my-org-atlantis-artifacts
    
    # ì„œë²„ë¦¬ìŠ¤ í”„ë¡œì íŠ¸
    $0 -p my-lambda -t serverless -u http://atlantis.example.com -b my-org-atlantis-artifacts

í”„ë¡œì íŠ¸ íƒ€ì…ë³„ í¬í•¨ ëª¨ë“ˆ:
    web-app    : VPC, EC2 (Auto Scaling), RDS, S3
    api        : VPC, ECS (Fargate), RDS, ElastiCache  
    serverless : Lambda, DynamoDB, S3, API Gateway
    custom     : ëŒ€í™”í˜• ëª¨ë“ˆ ì„ íƒ

ê²°ê³¼ë¬¼:
    - terraform/stacks/{PROJECT_NAME}/{ENV} ë””ë ‰í† ë¦¬ ìƒì„±
    - StackKit ëª¨ë“ˆì„ ì‚¬ìš©í•˜ëŠ” Terraform êµ¬ì„±
    - atlantis.yaml ì„¤ì • íŒŒì¼
    - í™˜ê²½ë³„ tfvars íŒŒì¼
    - README.md ë¬¸ì„œ
EOF
}

log_info() { echo -e "${BLUE}â„¹ï¸  $1${NC}"; }
log_success() { echo -e "${GREEN}âœ… $1${NC}"; }
log_warning() { echo -e "${YELLOW}âš ï¸  $1${NC}"; }
log_error() { echo -e "${RED}âŒ $1${NC}"; }

parse_args() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            -p|--project-name)
                PROJECT_NAME="$2"
                shift 2
                ;;
            -t|--type)
                PROJECT_TYPE="$2"
                shift 2
                ;;
            -u|--atlantis-url)
                ATLANTIS_URL="$2"
                shift 2
                ;;
            -b|--s3-bucket)
                ATLANTIS_S3_BUCKET="$2"
                shift 2
                ;;
            -e|--environment)
                ENVIRONMENT="$2"
                shift 2
                ;;
            --region)
                AWS_REGION="$2"
                shift 2
                ;;
            --auto)
                AUTO_SETUP=true
                shift
                ;;
            -h|--help)
                show_usage
                exit 0
                ;;
            *)
                log_error "Unknown option: $1"
                show_usage
                exit 1
                ;;
        esac
    done
}

validate_inputs() {
    log_info "ì…ë ¥ ê°’ ê²€ì¦ ì¤‘..."
    
    if [[ -z "$PROJECT_NAME" ]]; then
        log_error "í”„ë¡œì íŠ¸ ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤ (-p ì˜µì…˜ ì‚¬ìš©)"
        exit 1
    fi
    
    if [[ -z "$ATLANTIS_URL" ]]; then
        log_error "ì¤‘ì•™ Atlantis ì„œë²„ URLì´ í•„ìš”í•©ë‹ˆë‹¤ (-u ì˜µì…˜ ì‚¬ìš©)"
        exit 1
    fi
    
    if [[ -z "$ATLANTIS_S3_BUCKET" ]]; then
        log_error "Atlantis S3 ë²„í‚· ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤ (-b ì˜µì…˜ ì‚¬ìš©)"
        exit 1
    fi
    
    # Validate project type
    if [[ -n "$PROJECT_TYPE" ]] && [[ ! "$PROJECT_TYPE" =~ ^(web-app|api|serverless|custom)$ ]]; then
        log_error "ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œì íŠ¸ íƒ€ì…: $PROJECT_TYPE"
        log_error "ì§€ì› íƒ€ì…: web-app, api, serverless, custom"
        exit 1
    fi

    # Check required tools
    for tool in aws terraform jq; do
        if ! command -v $tool &> /dev/null; then
            log_error "$toolì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
            exit 1
        fi
    done

    # Check AWS credentials  
    if ! aws sts get-caller-identity &> /dev/null; then
        log_error "AWS ìê²©ì¦ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
        exit 1
    fi

    log_success "ëª¨ë“  ì…ë ¥ ê°’ì´ ìœ íš¨í•©ë‹ˆë‹¤"
}

interactive_setup() {
    if [[ "$AUTO_SETUP" == "true" ]]; then
        return
    fi

    log_info "ëŒ€í™”í˜• ì„¤ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
    
    if [[ -z "$PROJECT_NAME" ]]; then
        read -p "í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: " PROJECT_NAME
    fi
    
    if [[ -z "$PROJECT_TYPE" ]]; then
        echo "í”„ë¡œì íŠ¸ íƒ€ì…ì„ ì„ íƒí•˜ì„¸ìš”:"
        echo "1) web-app    - VPC + EC2 + RDS (ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜)"
        echo "2) api        - VPC + ECS + RDS + ElastiCache (API ì„œë²„)"  
        echo "3) serverless - Lambda + DynamoDB + S3 (ì„œë²„ë¦¬ìŠ¤)"
        echo "4) custom     - ëª¨ë“ˆ ì§ì ‘ ì„ íƒ"
        
        while true; do
            read -p "ì„ íƒ (1-4): " choice
            case $choice in
                1) PROJECT_TYPE="web-app"; break;;
                2) PROJECT_TYPE="api"; break;;
                3) PROJECT_TYPE="serverless"; break;;
                4) PROJECT_TYPE="custom"; break;;
                *) echo "ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.";;
            esac
        done
    fi
    
    if [[ -z "$ATLANTIS_URL" ]]; then
        read -p "ì¤‘ì•™ Atlantis ì„œë²„ URLì„ ì…ë ¥í•˜ì„¸ìš”: " ATLANTIS_URL
    fi
    
    if [[ -z "$ATLANTIS_S3_BUCKET" ]]; then
        read -p "Atlantis S3 ë²„í‚· ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: " ATLANTIS_S3_BUCKET
    fi
}

create_project_structure() {
    log_info "í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„± ì¤‘..."
    
    PROJECT_DIR="terraform/stacks/${PROJECT_NAME}/${ENVIRONMENT}"
    mkdir -p "${PROJECT_DIR}"
    
    log_success "í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±: ${PROJECT_DIR}"
}

generate_terraform_config() {
    log_info "Terraform êµ¬ì„± ìƒì„± ì¤‘..."
    
    PROJECT_DIR="terraform/stacks/${PROJECT_NAME}/${ENVIRONMENT}"
    
    # Backend configuration
    cat > "${PROJECT_DIR}/backend.hcl" << EOF
bucket         = "stackkit-tfstate-${AWS_REGION}-\$(aws sts get-caller-identity --query Account --output text)"
key            = "${PROJECT_NAME}/${ENVIRONMENT}/terraform.tfstate"
region         = "${AWS_REGION}"
encrypt        = true
dynamodb_table = "stackkit-tf-lock"
EOF

    # Variables
    cat > "${PROJECT_DIR}/variables.tf" << 'EOF'
variable "project_name" {
  description = "Project name"
  type        = string
}

variable "environment" {
  description = "Environment name"
  type        = string
}

variable "region" {
  description = "AWS region"
  type        = string
}

variable "common_tags" {
  description = "Common tags for all resources"
  type        = map(string)
  default     = {}
}
EOF

    # Generate main.tf based on project type
    case "$PROJECT_TYPE" in
        "web-app")
            generate_webapp_config
            ;;
        "api")
            generate_api_config
            ;;
        "serverless")
            generate_serverless_config
            ;;
        "custom")
            generate_custom_config
            ;;
        *)
            log_error "Unsupported project type: $PROJECT_TYPE"
            exit 1
            ;;
    esac
    
    # Terraform variables file
    cat > "${PROJECT_DIR}/terraform.tfvars" << EOF
project_name = "${PROJECT_NAME}"
environment  = "${ENVIRONMENT}"
region       = "${AWS_REGION}"

common_tags = {
  Project     = "${PROJECT_NAME}"
  Environment = "${ENVIRONMENT}"
  ManagedBy   = "terraform"
  CreatedBy   = "stackkit-setup"
}
EOF

    # Outputs
    cat > "${PROJECT_DIR}/outputs.tf" << 'EOF'
# Common outputs will be added based on project type
EOF
}

generate_webapp_config() {
    PROJECT_DIR="terraform/stacks/${PROJECT_NAME}/${ENVIRONMENT}"
    
    cat > "${PROJECT_DIR}/main.tf" << 'EOF'
terraform {
  required_version = ">= 1.7.0"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  
  backend "s3" {}
}

provider "aws" {
  region = var.region
}

locals {
  common_tags = merge(var.common_tags, {
    ProjectType = "web-app"
  })
}

# VPC Module
module "vpc" {
  source = "../../../modules/vpc"
  
  project_name = var.project_name
  environment  = var.environment
  vpc_cidr     = "10.0.0.0/16"
  
  availability_zones = ["${var.region}a", "${var.region}c"]
  public_subnet_cidrs  = ["10.0.1.0/24", "10.0.2.0/24"]
  private_subnet_cidrs = ["10.0.10.0/24", "10.0.20.0/24"]
  
  enable_nat_gateway = true
  single_nat_gateway = var.environment == "prod" ? false : true
  
  common_tags = local.common_tags
}

# EC2 Web Servers Module  
module "web_servers" {
  source = "../../../modules/ec2"
  
  project_name = var.project_name
  environment  = var.environment
  instance_type = var.environment == "prod" ? "t3.medium" : "t3.micro"
  
  # Auto Scaling Configuration
  min_size         = var.environment == "prod" ? 2 : 1
  max_size         = var.environment == "prod" ? 10 : 3
  desired_capacity = var.environment == "prod" ? 2 : 1
  
  # Networking
  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.public_subnet_ids
  
  # Security Group Rules
  security_group_rules = [
    {
      type        = "ingress"
      from_port   = 80
      to_port     = 80
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
      description = "HTTP access"
    },
    {
      type        = "ingress" 
      from_port   = 443
      to_port     = 443
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
      description = "HTTPS access"
    }
  ]
  
  common_tags = local.common_tags
}

# RDS Database Module
module "database" {
  source = "../../../modules/rds"
  
  project_name = var.project_name
  environment  = var.environment
  
  # Database Configuration
  engine         = "mysql"
  engine_version = "8.0"
  instance_class = var.environment == "prod" ? "db.t3.small" : "db.t3.micro"
  allocated_storage = var.environment == "prod" ? 100 : 20
  
  # Database Settings
  db_name  = replace(var.project_name, "-", "_")
  username = "admin"
  
  # High Availability
  multi_az = var.environment == "prod" ? true : false
  backup_retention_period = var.environment == "prod" ? 7 : 1
  backup_window = "03:00-04:00"
  
  # Networking
  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnet_ids
  
  # Security
  allowed_security_groups = [module.web_servers.security_group_id]
  
  common_tags = local.common_tags
}

# S3 Static Assets Module
module "static_assets" {
  source = "../../../modules/s3"
  
  project_name = var.project_name
  environment  = var.environment
  bucket_name  = "static-assets"
  
  # Enable static website hosting
  enable_static_website = true
  
  # CORS for web applications
  cors_rules = [
    {
      allowed_headers = ["*"]
      allowed_methods = ["GET", "HEAD"]
      allowed_origins = ["*"]
      max_age_seconds = 3600
    }
  ]
  
  common_tags = local.common_tags
}
EOF

    # Add web app specific outputs
    cat > "${PROJECT_DIR}/outputs.tf" << 'EOF'
output "vpc_id" {
  description = "VPC ID"
  value       = module.vpc.vpc_id
}

output "load_balancer_dns" {
  description = "Load balancer DNS name"
  value       = module.web_servers.load_balancer_dns_name
}

output "database_endpoint" {
  description = "RDS database endpoint"
  value       = module.database.endpoint
}

output "static_assets_bucket" {
  description = "S3 static assets bucket name"
  value       = module.static_assets.bucket_id
}

output "static_website_endpoint" {
  description = "S3 static website endpoint"
  value       = module.static_assets.website_endpoint
}
EOF
}

generate_api_config() {
    PROJECT_DIR="terraform/stacks/${PROJECT_NAME}/${ENVIRONMENT}"
    
    cat > "${PROJECT_DIR}/main.tf" << 'EOF'
terraform {
  required_version = ">= 1.7.0"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  
  backend "s3" {}
}

provider "aws" {
  region = var.region
}

locals {
  common_tags = merge(var.common_tags, {
    ProjectType = "api"
  })
}

# VPC Module
module "vpc" {
  source = "../../../modules/vpc"
  
  project_name = var.project_name
  environment  = var.environment
  vpc_cidr     = "10.0.0.0/16"
  
  availability_zones = ["${var.region}a", "${var.region}c"]
  public_subnet_cidrs  = ["10.0.1.0/24", "10.0.2.0/24"]
  private_subnet_cidrs = ["10.0.10.0/24", "10.0.20.0/24"]
  
  enable_nat_gateway = true
  single_nat_gateway = var.environment == "prod" ? false : true
  
  common_tags = local.common_tags
}

# ECS API Service Module
module "api_service" {
  source = "../../../modules/ecs"
  
  project_name = var.project_name
  environment  = var.environment
  
  cluster_name = "api-cluster"
  
  services = [
    {
      name         = "api"
      desired_count = var.environment == "prod" ? 3 : 1
      
      container_definitions = [
        {
          name  = "api"
          image = "${var.project_name}:latest"
          
          portMappings = [
            {
              containerPort = 8080
              protocol      = "tcp"
            }
          ]
          
          environment = [
            {
              name  = "ENVIRONMENT"
              value = var.environment
            },
            {
              name  = "AWS_REGION"
              value = var.region
            }
          ]
          
          essential = true
          
          logConfiguration = {
            logDriver = "awslogs"
            options = {
              awslogs-group         = "/ecs/${var.project_name}-${var.environment}"
              awslogs-region        = var.region
              awslogs-stream-prefix = "ecs"
            }
          }
        }
      ]
      
      cpu    = var.environment == "prod" ? 1024 : 512
      memory = var.environment == "prod" ? 2048 : 1024
      
      subnets         = module.vpc.private_subnet_ids
      security_groups = [aws_security_group.api.id]
      
      load_balancer = {
        target_group_arn = aws_lb_target_group.api.arn
        container_name   = "api"
        container_port   = 8080
      }
    }
  ]
  
  common_tags = local.common_tags
}

# RDS Database Module
module "database" {
  source = "../../../modules/rds"
  
  project_name = var.project_name
  environment  = var.environment
  
  engine         = "postgresql"
  engine_version = "15.4"
  instance_class = var.environment == "prod" ? "db.t3.medium" : "db.t3.micro"
  allocated_storage = var.environment == "prod" ? 100 : 20
  
  db_name  = replace(var.project_name, "-", "_")
  username = "admin"
  
  multi_az = var.environment == "prod" ? true : false
  backup_retention_period = var.environment == "prod" ? 7 : 1
  
  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnet_ids
  
  allowed_security_groups = [aws_security_group.api.id]
  
  common_tags = local.common_tags
}

# ElastiCache Redis Module
module "cache" {
  source = "../../../modules/elasticache"
  
  project_name = var.project_name
  environment  = var.environment
  
  engine         = "redis"
  node_type      = var.environment == "prod" ? "cache.t3.medium" : "cache.t3.micro"
  num_cache_nodes = var.environment == "prod" ? 3 : 1
  
  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnet_ids
  
  allowed_security_groups = [aws_security_group.api.id]
  
  common_tags = local.common_tags
}

# Security Groups
resource "aws_security_group" "api" {
  name_prefix = "${var.project_name}-${var.environment}-api"
  vpc_id      = module.vpc.vpc_id
  
  ingress {
    from_port       = 8080
    to_port         = 8080
    protocol        = "tcp"
    security_groups = [aws_security_group.alb.id]
  }
  
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  tags = merge(local.common_tags, {
    Name = "${var.project_name}-${var.environment}-api"
  })
}

resource "aws_security_group" "alb" {
  name_prefix = "${var.project_name}-${var.environment}-alb"
  vpc_id      = module.vpc.vpc_id
  
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  tags = merge(local.common_tags, {
    Name = "${var.project_name}-${var.environment}-alb"
  })
}

# Application Load Balancer
resource "aws_lb" "api" {
  name               = "${var.project_name}-${var.environment}-alb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.alb.id]
  subnets            = module.vpc.public_subnet_ids
  
  enable_deletion_protection = var.environment == "prod" ? true : false
  
  tags = local.common_tags
}

resource "aws_lb_target_group" "api" {
  name     = "${var.project_name}-${var.environment}-tg"
  port     = 8080
  protocol = "HTTP"
  vpc_id   = module.vpc.vpc_id
  
  target_type = "ip"
  
  health_check {
    enabled             = true
    healthy_threshold   = 2
    unhealthy_threshold = 3
    timeout             = 10
    interval            = 30
    path                = "/health"
    matcher             = "200"
  }
  
  tags = local.common_tags
}

resource "aws_lb_listener" "api" {
  load_balancer_arn = aws_lb.api.arn
  port              = "80"
  protocol          = "HTTP"
  
  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.api.arn
  }
}

resource "aws_cloudwatch_log_group" "api" {
  name              = "/ecs/${var.project_name}-${var.environment}"
  retention_in_days = var.environment == "prod" ? 30 : 7
  
  tags = local.common_tags
}
EOF

    # Add API specific outputs
    cat > "${PROJECT_DIR}/outputs.tf" << 'EOF'
output "vpc_id" {
  description = "VPC ID"
  value       = module.vpc.vpc_id
}

output "api_endpoint" {
  description = "API load balancer DNS name"
  value       = "http://${aws_lb.api.dns_name}"
}

output "database_endpoint" {
  description = "RDS database endpoint"  
  value       = module.database.endpoint
}

output "redis_endpoint" {
  description = "ElastiCache Redis endpoint"
  value       = module.cache.redis_endpoint
}

output "ecs_cluster_name" {
  description = "ECS cluster name"
  value       = module.api_service.cluster_name
}
EOF
}

generate_serverless_config() {
    PROJECT_DIR="terraform/stacks/${PROJECT_NAME}/${ENVIRONMENT}"
    
    cat > "${PROJECT_DIR}/main.tf" << 'EOF'
terraform {
  required_version = ">= 1.7.0"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  
  backend "s3" {}
}

provider "aws" {
  region = var.region
}

locals {
  common_tags = merge(var.common_tags, {
    ProjectType = "serverless"
  })
}

# DynamoDB Table Module
module "main_table" {
  source = "../../../modules/dynamodb"
  
  project_name = var.project_name
  environment  = var.environment
  table_name   = "main"
  
  hash_key = "pk"
  range_key = "sk"
  
  attributes = [
    {
      name = "pk"
      type = "S"
    },
    {
      name = "sk"
      type = "S"
    },
    {
      name = "gsi1pk"
      type = "S"
    },
    {
      name = "gsi1sk"
      type = "S"
    }
  ]
  
  global_secondary_indexes = [
    {
      name            = "GSI1"
      hash_key        = "gsi1pk"
      range_key       = "gsi1sk"
      projection_type = "ALL"
    }
  ]
  
  billing_mode = var.environment == "prod" ? "PROVISIONED" : "PAY_PER_REQUEST"
  
  read_capacity  = var.environment == "prod" ? 5 : null
  write_capacity = var.environment == "prod" ? 5 : null
  
  common_tags = local.common_tags
}

# S3 Storage Module
module "storage" {
  source = "../../../modules/s3"
  
  project_name = var.project_name
  environment  = var.environment
  bucket_name  = "storage"
  
  # Enable versioning for prod
  enable_versioning = var.environment == "prod" ? true : false
  
  # Lifecycle rules
  lifecycle_rules = [
    {
      id     = "delete_old_versions"
      status = "Enabled"
      noncurrent_version_expiration = {
        days = 90
      }
    }
  ]
  
  # CORS for API access
  cors_rules = [
    {
      allowed_headers = ["*"]
      allowed_methods = ["GET", "POST", "PUT", "DELETE", "HEAD"]
      allowed_origins = ["*"]
      max_age_seconds = 3600
    }
  ]
  
  common_tags = local.common_tags
}

# Lambda Function Module - API Handler
module "api_handler" {
  source = "../../../modules/lambda"
  
  project_name  = var.project_name
  environment   = var.environment
  function_name = "api-handler"
  
  runtime     = "python3.11"
  handler     = "app.handler"
  filename    = "./lambda-code/api-handler.zip"
  memory_size = var.environment == "prod" ? 512 : 256
  timeout     = 30
  
  # Environment variables
  environment_variables = {
    DYNAMODB_TABLE = module.main_table.table_name
    S3_BUCKET      = module.storage.bucket_id
    ENVIRONMENT    = var.environment
    AWS_REGION     = var.region
  }
  
  # IAM permissions
  additional_iam_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "dynamodb:GetItem",
          "dynamodb:PutItem", 
          "dynamodb:UpdateItem",
          "dynamodb:DeleteItem",
          "dynamodb:Query",
          "dynamodb:Scan"
        ]
        Resource = [
          module.main_table.table_arn,
          "${module.main_table.table_arn}/index/*"
        ]
      },
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:PutObject",
          "s3:DeleteObject"
        ]
        Resource = "${module.storage.bucket_arn}/*"
      }
    ]
  })
  
  common_tags = local.common_tags
}

# Lambda Function Module - Background Worker
module "worker" {
  source = "../../../modules/lambda"
  
  project_name  = var.project_name
  environment   = var.environment
  function_name = "worker"
  
  runtime     = "python3.11"
  handler     = "worker.handler"
  filename    = "./lambda-code/worker.zip"
  memory_size = var.environment == "prod" ? 1024 : 512
  timeout     = 300
  
  # Environment variables
  environment_variables = {
    DYNAMODB_TABLE = module.main_table.table_name
    S3_BUCKET      = module.storage.bucket_id
    ENVIRONMENT    = var.environment
    AWS_REGION     = var.region
  }
  
  # IAM permissions (same as API handler)
  additional_iam_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "dynamodb:GetItem",
          "dynamodb:PutItem",
          "dynamodb:UpdateItem", 
          "dynamodb:DeleteItem",
          "dynamodb:Query",
          "dynamodb:Scan"
        ]
        Resource = [
          module.main_table.table_arn,
          "${module.main_table.table_arn}/index/*"
        ]
      },
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:PutObject",
          "s3:DeleteObject"
        ]
        Resource = "${module.storage.bucket_arn}/*"
      }
    ]
  })
  
  common_tags = local.common_tags
}

# API Gateway (optional - can be added later)
resource "aws_api_gateway_rest_api" "main" {
  name        = "${var.project_name}-${var.environment}-api"
  description = "API Gateway for ${var.project_name}"
  
  tags = local.common_tags
}

resource "aws_api_gateway_resource" "proxy" {
  rest_api_id = aws_api_gateway_rest_api.main.id
  parent_id   = aws_api_gateway_rest_api.main.root_resource_id
  path_part   = "{proxy+}"
}

resource "aws_api_gateway_method" "proxy" {
  rest_api_id   = aws_api_gateway_rest_api.main.id
  resource_id   = aws_api_gateway_resource.proxy.id
  http_method   = "ANY"
  authorization = "NONE"
}

resource "aws_api_gateway_integration" "lambda" {
  rest_api_id = aws_api_gateway_rest_api.main.id
  resource_id = aws_api_gateway_method.proxy.resource_id
  http_method = aws_api_gateway_method.proxy.http_method
  
  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = module.api_handler.invoke_arn
}

resource "aws_api_gateway_deployment" "main" {
  depends_on = [
    aws_api_gateway_integration.lambda,
  ]
  
  rest_api_id = aws_api_gateway_rest_api.main.id
  stage_name  = var.environment
  
  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_lambda_permission" "api_gateway" {
  statement_id  = "AllowExecutionFromAPIGateway"
  action        = "lambda:InvokeFunction"
  function_name = module.api_handler.function_name
  principal     = "apigateway.amazonaws.com"
  source_arn    = "${aws_api_gateway_rest_api.main.execution_arn}/*/*"
}
EOF

    # Add serverless specific outputs
    cat > "${PROJECT_DIR}/outputs.tf" << 'EOF'
output "dynamodb_table_name" {
  description = "DynamoDB table name"
  value       = module.main_table.table_name
}

output "s3_bucket_name" {
  description = "S3 storage bucket name"
  value       = module.storage.bucket_id
}

output "api_handler_function_name" {
  description = "API handler Lambda function name"
  value       = module.api_handler.function_name
}

output "worker_function_name" {
  description = "Worker Lambda function name"
  value       = module.worker.function_name
}

output "api_endpoint" {
  description = "API Gateway endpoint URL"
  value       = aws_api_gateway_deployment.main.invoke_url
}
EOF

    # Create sample Lambda code structure
    mkdir -p "lambda-code"
    
    # API Handler sample code
    cat > "lambda-code/app.py" << 'EOF'
import json
import boto3
import os
from decimal import Decimal

# Initialize AWS clients
dynamodb = boto3.resource('dynamodb')
s3 = boto3.client('s3')

# Environment variables
TABLE_NAME = os.environ['DYNAMODB_TABLE']
BUCKET_NAME = os.environ['S3_BUCKET']

table = dynamodb.Table(TABLE_NAME)

def handler(event, context):
    """
    Lambda API handler function
    """
    try:
        method = event.get('httpMethod', 'GET')
        path = event.get('path', '/')
        
        if method == 'GET' and path == '/health':
            return {
                'statusCode': 200,
                'body': json.dumps({
                    'status': 'healthy',
                    'service': 'api-handler'
                })
            }
        
        elif method == 'GET' and path.startswith('/items'):
            # List items from DynamoDB
            response = table.scan(Limit=10)
            items = response.get('Items', [])
            
            return {
                'statusCode': 200,
                'body': json.dumps(items, default=decimal_default)
            }
        
        elif method == 'POST' and path == '/items':
            # Create new item
            body = json.loads(event.get('body', '{}'))
            
            item = {
                'pk': f"ITEM#{body.get('id')}",
                'sk': 'METADATA',
                'data': body
            }
            
            table.put_item(Item=item)
            
            return {
                'statusCode': 201,
                'body': json.dumps({'message': 'Item created successfully'})
            }
        
        else:
            return {
                'statusCode': 404,
                'body': json.dumps({'error': 'Not found'})
            }
    
    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }

def decimal_default(obj):
    """JSON serializer for Decimal objects"""
    if isinstance(obj, Decimal):
        return float(obj)
    raise TypeError
EOF

    # Worker sample code
    cat > "lambda-code/worker.py" << 'EOF'
import json
import boto3
import os

# Initialize AWS clients
dynamodb = boto3.resource('dynamodb')
s3 = boto3.client('s3')

# Environment variables
TABLE_NAME = os.environ['DYNAMODB_TABLE']
BUCKET_NAME = os.environ['S3_BUCKET']

table = dynamodb.Table(TABLE_NAME)

def handler(event, context):
    """
    Lambda worker function for background processing
    """
    try:
        # Process each record in the event
        for record in event.get('Records', []):
            
            # Handle S3 events
            if 'aws:s3' in record.get('eventSource', ''):
                bucket = record['s3']['bucket']['name']
                key = record['s3']['object']['key']
                
                print(f"Processing S3 object: {bucket}/{key}")
                
                # Add processing logic here
                # Example: read file, process, update DynamoDB
                
            # Handle DynamoDB events
            elif 'aws:dynamodb' in record.get('eventSource', ''):
                event_name = record['eventName']
                
                print(f"Processing DynamoDB event: {event_name}")
                
                # Add processing logic here
                # Example: send notifications, trigger workflows
        
        return {
            'statusCode': 200,
            'body': json.dumps({'message': 'Processing completed successfully'})
        }
    
    except Exception as e:
        print(f"Error: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }
EOF

    # Create deployment packages
    cd lambda-code
    zip -q api-handler.zip app.py
    zip -q worker.zip worker.py  
    cd ..
    
    log_success "Lambda ì½”ë“œ ìƒ˜í”Œì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤"
}

generate_custom_config() {
    PROJECT_DIR="terraform/stacks/${PROJECT_NAME}/${ENVIRONMENT}"
    
    log_info "ì‚¬ìš©í•  ëª¨ë“ˆì„ ì„ íƒí•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„):"
    echo "ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆ: vpc, ec2, ecs, rds, lambda, dynamodb, s3, elasticache"
    
    read -p "ëª¨ë“ˆ ëª©ë¡: " modules_input
    
    IFS=',' read -ra MODULES <<< "$modules_input"
    
    # Trim whitespace
    for i in "${!MODULES[@]}"; do
        MODULES[i]=$(echo "${MODULES[i]}" | xargs)
    done
    
    cat > "${PROJECT_DIR}/main.tf" << 'EOF'
terraform {
  required_version = ">= 1.7.0"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  
  backend "s3" {}
}

provider "aws" {
  region = var.region
}

locals {
  common_tags = merge(var.common_tags, {
    ProjectType = "custom"
  })
}
EOF

    for module in "${MODULES[@]}"; do
        case "$module" in
            "vpc")
                cat >> "${PROJECT_DIR}/main.tf" << 'EOF'

# VPC Module
module "vpc" {
  source = "../../../modules/vpc"
  
  project_name = var.project_name
  environment  = var.environment
  vpc_cidr     = "10.0.0.0/16"
  
  availability_zones = ["${var.region}a", "${var.region}c"]
  public_subnet_cidrs  = ["10.0.1.0/24", "10.0.2.0/24"]
  private_subnet_cidrs = ["10.0.10.0/24", "10.0.20.0/24"]
  
  enable_nat_gateway = true
  single_nat_gateway = var.environment == "prod" ? false : true
  
  common_tags = local.common_tags
}
EOF
                ;;
            "ec2")
                cat >> "${PROJECT_DIR}/main.tf" << 'EOF'

# EC2 Module
module "ec2" {
  source = "../../../modules/ec2"
  
  project_name = var.project_name
  environment  = var.environment
  instance_type = var.environment == "prod" ? "t3.medium" : "t3.micro"
  
  min_size         = var.environment == "prod" ? 2 : 1
  max_size         = var.environment == "prod" ? 10 : 3
  desired_capacity = var.environment == "prod" ? 2 : 1
  
  # vpc_id and subnet_ids should reference VPC module if used
  # vpc_id     = module.vpc.vpc_id
  # subnet_ids = module.vpc.public_subnet_ids
  
  common_tags = local.common_tags
}
EOF
                ;;
            # Add other modules as needed...
        esac
    done
    
    log_success "ì»¤ìŠ¤í…€ ëª¨ë“ˆ êµ¬ì„±ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤"
}

create_atlantis_config() {
    log_info "atlantis.yaml ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘..."
    
    cat > "atlantis.yaml" << EOF
version: 3

parallel_plan: true
parallel_apply: true

projects:
  - name: ${PROJECT_NAME}-${ENVIRONMENT}
    dir: terraform/stacks/${PROJECT_NAME}/${ENVIRONMENT}
    workflow: stackkit-${ENVIRONMENT}
    autoplan:
      enabled: true
      when_modified: ["**/*.tf", "**/*.tfvars", "../../../modules/**"]
    terraform_version: v1.8.5

workflows:
  stackkit-${ENVIRONMENT}:
    plan:
      steps:
        - init
        - plan:
            extra_args: ["-input=false", "-var-file", "terraform.tfvars"]
        - run: |
            set -euo pipefail
            # Plan ê²°ê³¼ë¥¼ JSON/í…ìŠ¤íŠ¸ë¡œ ë¤í”„
            terraform show -json "\$PLANFILE" > tfplan.json
            terraform show "\$PLANFILE" > plan.txt
            
            # Infracost ë¹„ìš© ì¶”ì • ì‹¤í–‰
            echo "ğŸ’° Infracost ë¹„ìš© ì¶”ì • ì‹¤í–‰ ì¤‘..."
            if command -v infracost &> /dev/null; then
                if [[ -z "\${INFRACOST_API_KEY:-}" ]]; then
                    echo "âš ï¸  INFRACOST_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ. ê¸°ë³¸ ë¹„ìš© ë°ì´í„° ìƒì„±."
                    echo '{"totalMonthlyCost":"0","currency":"USD","projects":[]}' > infracost.json
                else
                    infracost breakdown \\
                        --path . \\
                        --format json \\
                        --out-file infracost.json \\
                        --terraform-plan-path "\$PLANFILE" || {
                        echo "âš ï¸  Infracost ì‹¤í–‰ ì‹¤íŒ¨. ê¸°ë³¸ ë¹„ìš© ë°ì´í„° ìƒì„±."
                        echo '{"totalMonthlyCost":"0","currency":"USD","projects":[]}' > infracost.json
                    }
                fi
            else
                echo "âš ï¸  Infracostê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ê¸°ë³¸ ë¹„ìš© ë°ì´í„° ìƒì„±."
                echo '{"totalMonthlyCost":"0","currency":"USD","projects":[]}' > infracost.json
            fi
            
            # ë³€ê²½ ìœ ë¬´ ê³„ì‚°
            HAS_CHANGES=\$(jq '(.resource_changes|length) > 0' tfplan.json)
            
            # ë¹„ìš© ì •ë³´ ì¶”ì¶œ
            MONTHLY_COST=\$(jq -r '.totalMonthlyCost // "0"' infracost.json 2>/dev/null || echo "0")
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            jq -n --arg repo "\${BASE_REPO_OWNER}/\${BASE_REPO_NAME}" \\
                  --arg pr "\$PULL_NUM" \\
                  --arg proj "${PROJECT_NAME}" \\
                  --arg action "plan" \\
                  --arg status "success" \\
                  --arg commit "\$HEAD_COMMIT" \\
                  --arg cost "\$MONTHLY_COST" \\
                  --argjson has "\$HAS_CHANGES" \\
                  '{repo:\$repo,pr:(\$pr|tonumber),project:\$proj,action:\$action,status:\$status,commit:\$commit,has_changes:\$has,monthly_cost:\$cost}' \\
              > manifest.json
            
            # S3 ì—…ë¡œë“œ ê²½ë¡œ êµ¬ì„±
            BUCKET="${ATLANTIS_S3_BUCKET}"
            PREFIX="atlantis/\${BASE_REPO_OWNER}/\${BASE_REPO_NAME}/\${PULL_NUM}/${PROJECT_NAME}"
            
            # S3 ì—…ë¡œë“œ
            aws s3 cp "\$PLANFILE"      "s3://\$BUCKET/\$PREFIX/tfplan.bin"
            aws s3 cp tfplan.json      "s3://\$BUCKET/\$PREFIX/tfplan.json"
            aws s3 cp plan.txt         "s3://\$BUCKET/\$PREFIX/plan.txt"
            aws s3 cp infracost.json   "s3://\$BUCKET/\$PREFIX/infracost.json"
            aws s3 cp manifest.json    "s3://\$BUCKET/\$PREFIX/manifest.json"
            
            echo "ğŸ“¤ Plan ê²°ê³¼ê°€ ì¤‘ì•™ AI ë¦¬ë·°ì–´ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤"
            echo "ğŸ’° ì˜ˆìƒ ì›”ê°„ ë¹„ìš©: \$MONTHLY_COST USD"
    apply:
      steps:
        - run: |
            set +e
            terraform apply -input=false -no-color "\$PLANFILE" | tee apply.txt
            STATUS=\$?
            set -e
            
            # Apply ê²°ê³¼ ë©”íƒ€ë°ì´í„°
            BUCKET="${ATLANTIS_S3_BUCKET}"
            PREFIX="atlantis/\${BASE_REPO_OWNER}/\${BASE_REPO_NAME}/\${PULL_NUM}/${PROJECT_NAME}"
            
            jq -n --arg repo "\${BASE_REPO_OWNER}/\${BASE_REPO_NAME}" \\
                  --arg pr "\$PULL_NUM" \\
                  --arg proj "${PROJECT_NAME}" \\
                  --arg action "apply" \\
                  --arg status "\$([ \$STATUS -eq 0 ] && echo success || echo failure)" \\
                  --arg commit "\$HEAD_COMMIT" \\
                  '{repo:\$repo,pr:(\$pr|tonumber),project:\$proj,action:\$action,status:\$status,commit:\$commit}' \\
              > manifest.json
              
            aws s3 cp apply.txt      "s3://\$BUCKET/\$PREFIX/apply.txt"
            aws s3 cp manifest.json  "s3://\$BUCKET/\$PREFIX/manifest.json"
            
            exit \$STATUS
EOF

    log_success "atlantis.yaml ì„¤ì • íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤"
}

create_project_readme() {
    log_info "í”„ë¡œì íŠ¸ README.md ìƒì„± ì¤‘..."
    
    cat > "README.md" << EOF
# ${PROJECT_NAME}

${PROJECT_TYPE} í”„ë¡œì íŠ¸ - StackKit ëª¨ë“ˆì„ ì‚¬ìš©í•œ AWS ì¸í”„ë¼

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

\`\`\`
${PROJECT_NAME}/
â”œâ”€â”€ terraform/
â”‚   â””â”€â”€ stacks/
â”‚       â””â”€â”€ ${PROJECT_NAME}/
â”‚           â””â”€â”€ ${ENVIRONMENT}/
â”‚               â”œâ”€â”€ main.tf           # ë©”ì¸ Terraform êµ¬ì„±
â”‚               â”œâ”€â”€ variables.tf      # ë³€ìˆ˜ ì •ì˜
â”‚               â”œâ”€â”€ outputs.tf        # ì¶œë ¥ ê°’
â”‚               â”œâ”€â”€ terraform.tfvars  # í™˜ê²½ë³„ ë³€ìˆ˜ ê°’
â”‚               â””â”€â”€ backend.hcl       # ë°±ì—”ë“œ ì„¤ì •
â”œâ”€â”€ atlantis.yaml                     # Atlantis ì„¤ì •
â””â”€â”€ README.md                         # ì´ íŒŒì¼
\`\`\`

## ğŸš€ ë°°í¬ ë°©ë²•

### ë¡œì»¬ ë°°í¬
\`\`\`bash
cd terraform/stacks/${PROJECT_NAME}/${ENVIRONMENT}

# Terraform ì´ˆê¸°í™”
terraform init -backend-config=backend.hcl

# ê³„íš í™•ì¸
terraform plan

# ë°°í¬ ì‹¤í–‰
terraform apply
\`\`\`

### Atlantisë¥¼ í†µí•œ ë°°í¬
1. Terraform íŒŒì¼ ìˆ˜ì •
2. Pull Request ìƒì„±
3. Atlantisê°€ ìë™ìœ¼ë¡œ \`terraform plan\` ì‹¤í–‰
4. AIê°€ Planì„ ë¶„ì„í•˜ì—¬ Slackì— ë¦¬ë·° ê²°ê³¼ ì „ì†¡
5. ê²€í†  í›„ PR ì½”ë©˜íŠ¸ì— \`atlantis apply\` ì…ë ¥

## ğŸ—ï¸ ì¸í”„ë¼ êµ¬ì„±

### í”„ë¡œì íŠ¸ íƒ€ì…: ${PROJECT_TYPE}

EOF

    case "$PROJECT_TYPE" in
        "web-app")
            cat >> "README.md" << 'EOF'
#### í¬í•¨ëœ AWS ë¦¬ì†ŒìŠ¤:
- **VPC**: ë„¤íŠ¸ì›Œí‚¹ ê¸°ë°˜ (10.0.0.0/16)
- **EC2**: Auto Scaling Group + Application Load Balancer  
- **RDS**: MySQL ë°ì´í„°ë² ì´ìŠ¤ (Multi-AZ for prod)
- **S3**: ì •ì  ìì‚° ì €ì¥ì†Œ (ì›¹ì‚¬ì´íŠ¸ í˜¸ìŠ¤íŒ… ì§€ì›)

#### ì£¼ìš” ì¶œë ¥ê°’:
- `load_balancer_dns`: ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì ‘ì† URL
- `database_endpoint`: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì—”ë“œí¬ì¸íŠ¸
- `static_website_endpoint`: ì •ì  ì›¹ì‚¬ì´íŠ¸ URL
EOF
            ;;
        "api")
            cat >> "README.md" << 'EOF'
#### í¬í•¨ëœ AWS ë¦¬ì†ŒìŠ¤:
- **VPC**: ë„¤íŠ¸ì›Œí‚¹ ê¸°ë°˜ (10.0.0.0/16)
- **ECS**: Fargateë¥¼ ì‚¬ìš©í•œ ì»¨í…Œì´ë„ˆ ì„œë¹„ìŠ¤
- **RDS**: PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ (Multi-AZ for prod)
- **ElastiCache**: Redis ìºì‹œ í´ëŸ¬ìŠ¤í„°

#### ì£¼ìš” ì¶œë ¥ê°’:
- `api_endpoint`: API ì ‘ì† URL
- `database_endpoint`: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì—”ë“œí¬ì¸íŠ¸  
- `redis_endpoint`: Redis ìºì‹œ ì—”ë“œí¬ì¸íŠ¸
EOF
            ;;
        "serverless")
            cat >> "README.md" << 'EOF'
#### í¬í•¨ëœ AWS ë¦¬ì†ŒìŠ¤:
- **Lambda**: API í•¸ë“¤ëŸ¬ ë° ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ í•¨ìˆ˜
- **DynamoDB**: NoSQL ë°ì´í„°ë² ì´ìŠ¤ (GSI í¬í•¨)
- **S3**: íŒŒì¼ ì €ì¥ì†Œ (CORS ì„¤ì • í¬í•¨)
- **API Gateway**: REST API ì—”ë“œí¬ì¸íŠ¸

#### ì£¼ìš” ì¶œë ¥ê°’:
- `api_endpoint`: API Gateway URL
- `dynamodb_table_name`: DynamoDB í…Œì´ë¸” ì´ë¦„
- `s3_bucket_name`: S3 ë²„í‚· ì´ë¦„
EOF
            ;;
    esac

    cat >> "README.md" << 'EOF'

## ğŸ”§ í™˜ê²½ë³„ ì„¤ì •

### Development (dev)
- ë¹„ìš© ìµœì í™” (t3.micro, ë‹¨ì¼ AZ)
- ë‹¨ìˆœí™”ëœ ì„¤ì •
- ì§§ì€ ë°±ì—… ë³´ê´€ ê¸°ê°„

### Production (prod)  
- ê³ ê°€ìš©ì„± (Multi-AZ)
- í–¥ìƒëœ ì„±ëŠ¥ ì¸ìŠ¤í„´ìŠ¤
- ì‚­ì œ ë³´í˜¸ í™œì„±í™”
- ê¸´ ë°±ì—… ë³´ê´€ ê¸°ê°„

## ğŸ“Š ë¹„ìš© ì¶”ì •

ëª¨ë“  Terraform Planì€ Infracostë¥¼ í†µí•´ ìë™ìœ¼ë¡œ ë¹„ìš©ì´ ê³„ì‚°ë˜ë©°, AI ë¦¬ë·°ì–´ê°€ ë¹„ìš© ì˜í–¥ì„ ë¶„ì„í•˜ì—¬ Slackìœ¼ë¡œ ì•Œë¦¼ì„ ë³´ëƒ…ë‹ˆë‹¤.

## ğŸ¤– AI ë¦¬ë·° í”„ë¡œì„¸ìŠ¤

1. **Plan ì‹¤í–‰**: PR ìƒì„± ì‹œ ìë™ìœ¼ë¡œ `terraform plan` ì‹¤í–‰
2. **AI ë¶„ì„**: GPT-4ê°€ Plan ê²°ê³¼ë¥¼ ë¶„ì„
3. **Slack ì•Œë¦¼**: ìƒì„¸í•œ ë¶„ì„ ê²°ê³¼ì™€ ê¶Œì¥ì‚¬í•­ ì „ì†¡
4. **ê²€í† **: íŒ€ì—ì„œ ë¦¬ë·° í›„ approve/apply

## ğŸ†˜ ë¬¸ì œ í•´ê²°

### Terraform ì˜¤ë¥˜
```bash
# ìƒíƒœ íŒŒì¼ í™•ì¸
terraform state list

# íŠ¹ì • ë¦¬ì†ŒìŠ¤ ë‹¤ì‹œ ìƒì„±
terraform apply -replace=module.vpc.aws_vpc.main
```

### Atlantis ê´€ë ¨ ë¬¸ì œ
- Atlantis ì„œë²„ ë¡œê·¸: CloudWatch `/ecs/atlantis-{org-name}`
- AI ë¦¬ë·°ì–´ ë¡œê·¸: CloudWatch `/aws/lambda/atlantis-{org-name}-central-unified-reviewer-handler`

## ğŸ”— ê´€ë ¨ ë§í¬

- [StackKit ë©”ì¸ ë¬¸ì„œ](../../../README.md)
- [ì¤‘ì•™ Atlantis ì„œë²„ ê´€ë¦¬](../../../ATLANTIS_SETUP.md)
- [StackKit ëª¨ë“ˆ ë¬¸ì„œ](../../../terraform/modules/)
EOF

    log_success "í”„ë¡œì íŠ¸ README.mdê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤"
}

show_completion_summary() {
    echo
    log_success "ğŸ‰ í”„ë¡œì íŠ¸ ì„¤ì • ì™„ë£Œ!"
    echo
    log_info "ìƒì„±ëœ íŒŒì¼ë“¤:"
    log_info "- terraform/stacks/${PROJECT_NAME}/${ENVIRONMENT}/ (Terraform êµ¬ì„±)"
    log_info "- atlantis.yaml (Atlantis ì„¤ì •)"
    log_info "- README.md (í”„ë¡œì íŠ¸ ë¬¸ì„œ)"
    
    if [[ "$PROJECT_TYPE" == "serverless" ]]; then
        log_info "- lambda-code/ (Lambda í•¨ìˆ˜ ìƒ˜í”Œ ì½”ë“œ)"
    fi
    
    echo
    log_info "ë‹¤ìŒ ë‹¨ê³„:"
    log_info "1. GitHub Repositoryì— Webhook ì„¤ì •:"
    log_info "   - URL: ${ATLANTIS_URL}/events" 
    log_info "   - Events: Pull requests, Issue comments, Push"
    log_info "2. ì½”ë“œë¥¼ Gitì— ì»¤ë°‹í•˜ê³  PR ìƒì„±"
    log_info "3. Atlantisê°€ ìë™ìœ¼ë¡œ Plan ì‹¤í–‰í•˜ê³  AI ë¦¬ë·° ì§„í–‰"
    
    echo
    log_success "ğŸš€ ì´ì œ ì•ˆì „í•˜ê³  ë˜‘ë˜‘í•œ ì¸í”„ë¼ ê´€ë¦¬ë¥¼ ì‹œì‘í•˜ì„¸ìš”!"
}

main() {
    echo -e "${BLUE}ğŸš€ StackKit í”„ë¡œì íŠ¸ ë ˆí¬ì§€í† ë¦¬ ì„¤ì •${NC}"
    echo -e "${BLUE}===========================================${NC}"
    echo
    
    parse_args "$@"
    
    if [[ "${#@}" -eq 0 ]]; then
        show_usage
        exit 0
    fi
    
    interactive_setup
    validate_inputs
    
    echo
    log_info "ì„¤ì • ìš”ì•½:"
    log_info "- í”„ë¡œì íŠ¸: ${PROJECT_NAME}"
    log_info "- íƒ€ì…: ${PROJECT_TYPE}"
    log_info "- í™˜ê²½: ${ENVIRONMENT}"
    log_info "- Atlantis: ${ATLANTIS_URL}"
    echo
    
    if [[ "$AUTO_SETUP" == "false" ]]; then
        read -p "ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? [y/N]: " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            log_warning "ì„¤ì •ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤"
            exit 0
        fi
    fi
    
    create_project_structure
    generate_terraform_config
    create_atlantis_config
    create_project_readme
    show_completion_summary
}

if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi