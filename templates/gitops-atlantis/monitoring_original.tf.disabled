# =======================================
# CloudWatch Monitoring and Alerting
# =======================================
# Enhanced monitoring for Atlantis ECS service with Slack integration

# CloudWatch Dashboard for Atlantis Monitoring
resource "aws_cloudwatch_dashboard" "atlantis_dashboard" {
  dashboard_name = "${local.name_prefix}-dashboard"

  dashboard_body = jsonencode({
    widgets = [
      {
        type   = "metric"
        x      = 0
        y      = 0
        width  = 12
        height = 6
        properties = {
          metrics = [
            ["AWS/ECS", "CPUUtilization", "ServiceName", aws_ecs_service.atlantis.name, "ClusterName", aws_ecs_cluster.main.name],
            [".", "MemoryUtilization", ".", ".", ".", "."],
            ["AWS/ApplicationELB", "TargetResponseTime", "LoadBalancer", aws_lb.atlantis.arn_suffix],
            [".", "RequestCount", ".", "."],
            [".", "HealthyHostCount", "TargetGroup", aws_lb_target_group.atlantis.arn_suffix]
          ]
          view    = "timeSeries"
          stacked = false
          region  = data.aws_region.current.name
          title   = "Atlantis Service Metrics"
          period  = 300
        }
      },
      {
        type   = "log"
        x      = 0
        y      = 6
        width  = 24
        height = 6
        properties = {
          query  = "SOURCE '/ecs/${local.name_prefix}'\n| fields @timestamp, @message\n| filter @message like /ERROR/\n| sort @timestamp desc\n| limit 50"
          region = data.aws_region.current.name
          title  = "Recent Errors"
        }
      }
    ]
  })
}

# SNS Topic for Atlantis Alerts
resource "aws_sns_topic" "atlantis_alerts" {
  name         = "${local.name_prefix}-alerts"
  display_name = "Atlantis Infrastructure Alerts"

  tags = local.common_tags
}

# SNS Topic Policy for CloudWatch Alarms
resource "aws_sns_topic_policy" "atlantis_alerts_policy" {
  arn = aws_sns_topic.atlantis_alerts.arn

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Principal = {
          Service = "cloudwatch.amazonaws.com"
        }
        Action   = "SNS:Publish"
        Resource = aws_sns_topic.atlantis_alerts.arn
        Condition = {
          StringEquals = {
            "aws:SourceAccount" = data.aws_caller_identity.current.account_id
          }
        }
      }
    ]
  })
}

# Lambda function for Slack notifications
resource "aws_lambda_function" "slack_notifier" {
  count = var.slack_webhook_url != "" ? 1 : 0

  filename      = "slack_notifier.zip"
  function_name = "${local.name_prefix}-slack-notifier"
  role          = aws_iam_role.lambda_slack_role[0].arn
  handler       = "index.handler"
  runtime       = "python3.9"
  timeout       = 30

  environment {
    variables = {
      SLACK_WEBHOOK_URL = var.slack_webhook_url
      SERVICE_NAME      = local.name_prefix
      ATLANTIS_URL      = local.atlantis_url
    }
  }

  tags = local.common_tags

  depends_on = [
    aws_iam_role_policy_attachment.lambda_basic_execution[0],
    aws_cloudwatch_log_group.lambda_logs[0],
    data.archive_file.slack_notifier_zip[0]
  ]
}

# Lambda function zip file
data "archive_file" "slack_notifier_zip" {
  count = var.slack_webhook_url != "" ? 1 : 0

  type        = "zip"
  output_path = "slack_notifier.zip"
  source {
    content = templatefile("${path.module}/lambda/slack_notifier.py", {
      service_name = local.name_prefix
      atlantis_url = local.atlantis_url
    })
    filename = "index.py"
  }
}

# CloudWatch Log Group for Lambda
resource "aws_cloudwatch_log_group" "lambda_logs" {
  count = var.slack_webhook_url != "" ? 1 : 0

  name              = "/aws/lambda/${local.name_prefix}-slack-notifier"
  retention_in_days = var.log_retention_days

  tags = local.common_tags
}

# IAM Role for Lambda
resource "aws_iam_role" "lambda_slack_role" {
  count = var.slack_webhook_url != "" ? 1 : 0

  name = "${local.name_prefix}-lambda-slack-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "lambda.amazonaws.com"
        }
      }
    ]
  })

  tags = local.common_tags
}

# IAM Policy Attachment for Lambda Basic Execution
resource "aws_iam_role_policy_attachment" "lambda_basic_execution" {
  count = var.slack_webhook_url != "" ? 1 : 0

  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
  role       = aws_iam_role.lambda_slack_role[0].name
}

# SNS Subscription for Lambda
resource "aws_sns_topic_subscription" "slack_notification" {
  count = var.slack_webhook_url != "" ? 1 : 0

  topic_arn = aws_sns_topic.atlantis_alerts.arn
  protocol  = "lambda"
  endpoint  = aws_lambda_function.slack_notifier[0].arn
}

# Lambda Permission for SNS
resource "aws_lambda_permission" "allow_sns" {
  count = var.slack_webhook_url != "" ? 1 : 0

  statement_id  = "AllowExecutionFromSNS"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.slack_notifier[0].function_name
  principal     = "sns.amazonaws.com"
  source_arn    = aws_sns_topic.atlantis_alerts.arn
}

# Enhanced CloudWatch Alarms
resource "aws_cloudwatch_metric_alarm" "service_unhealthy" {
  alarm_name          = "${local.name_prefix}-service-unhealthy"
  comparison_operator = "LessThanThreshold"
  evaluation_periods  = 2
  metric_name         = "HealthyHostCount"
  namespace           = "AWS/ApplicationELB"
  period              = 60
  statistic           = "Average"
  threshold           = 1
  alarm_description   = "Atlantis service has no healthy targets"
  alarm_actions       = [aws_sns_topic.atlantis_alerts.arn]
  ok_actions          = [aws_sns_topic.atlantis_alerts.arn]

  dimensions = {
    TargetGroup  = aws_lb_target_group.atlantis.arn_suffix
    LoadBalancer = aws_lb.atlantis.arn_suffix
  }

  tags = local.common_tags
}

resource "aws_cloudwatch_metric_alarm" "high_response_time" {
  alarm_name          = "${local.name_prefix}-high-response-time"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 3
  metric_name         = "TargetResponseTime"
  namespace           = "AWS/ApplicationELB"
  period              = 300
  statistic           = "Average"
  threshold           = 5.0
  alarm_description   = "Atlantis response time is too high"
  alarm_actions       = [aws_sns_topic.atlantis_alerts.arn]
  ok_actions          = [aws_sns_topic.atlantis_alerts.arn]

  dimensions = {
    LoadBalancer = aws_lb.atlantis.arn_suffix
  }

  tags = local.common_tags
}

resource "aws_cloudwatch_metric_alarm" "ecs_service_task_count" {
  alarm_name          = "${local.name_prefix}-task-count-zero"
  comparison_operator = "LessThanThreshold"
  evaluation_periods  = 1
  metric_name         = "RunningTaskCount"
  namespace           = "AWS/ECS"
  period              = 60
  statistic           = "Average"
  threshold           = 1
  alarm_description   = "Atlantis ECS service has no running tasks"
  alarm_actions       = [aws_sns_topic.atlantis_alerts.arn]
  ok_actions          = [aws_sns_topic.atlantis_alerts.arn]

  dimensions = {
    ServiceName = aws_ecs_service.atlantis.name
    ClusterName = aws_ecs_cluster.main.name
  }

  tags = local.common_tags
}

resource "aws_cloudwatch_metric_alarm" "deployment_failure" {
  alarm_name          = "${local.name_prefix}-deployment-failure"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "FailedTaskCount"
  namespace           = "AWS/ECS"
  period              = 300
  statistic           = "Sum"
  threshold           = 0
  alarm_description   = "Atlantis deployment has failed tasks"
  alarm_actions       = [aws_sns_topic.atlantis_alerts.arn]
  treat_missing_data  = "notBreaching"

  dimensions = {
    ServiceName = aws_ecs_service.atlantis.name
    ClusterName = aws_ecs_cluster.main.name
  }

  tags = local.common_tags
}

# Application-specific alarms for Atlantis functionality
resource "aws_cloudwatch_metric_alarm" "atlantis_webhook_errors" {
  alarm_name          = "${local.name_prefix}-webhook-errors"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "HTTPCode_Target_4XX_Count"
  namespace           = "AWS/ApplicationELB"
  period              = 300
  statistic           = "Sum"
  threshold           = 10
  alarm_description   = "High number of 4xx errors on Atlantis webhooks"
  alarm_actions       = [aws_sns_topic.atlantis_alerts.arn]
  treat_missing_data  = "notBreaching"

  dimensions = {
    LoadBalancer = aws_lb.atlantis.arn_suffix
  }

  tags = local.common_tags
}

resource "aws_cloudwatch_metric_alarm" "container_restart" {
  count = var.enable_container_insights ? 1 : 0

  alarm_name          = "${local.name_prefix}-container-restart"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "ContainerInstanceCount"
  namespace           = "ECS/ContainerInsights"
  period              = 300
  statistic           = "Average"
  threshold           = 1
  alarm_description   = "Atlantis container has restarted unexpectedly"
  alarm_actions       = [aws_sns_topic.atlantis_alerts.arn]
  treat_missing_data  = "notBreaching"

  dimensions = {
    ServiceName = aws_ecs_service.atlantis.name
    ClusterName = aws_ecs_cluster.main.name
  }

  tags = local.common_tags
}

# Custom metric for VaultDB-specific monitoring
resource "aws_cloudwatch_log_metric_filter" "vaultdb_connection_errors" {
  name           = "${local.name_prefix}-vaultdb-connection-errors"
  log_group_name = aws_cloudwatch_log_group.atlantis.name
  pattern        = "\"VaultDB\" \"connection\" \"ERROR\""

  metric_transformation {
    name      = "VaultDBConnectionErrors"
    namespace = "Atlantis/VaultDB"
    value     = "1"
  }
}

resource "aws_cloudwatch_metric_alarm" "vaultdb_connection_errors" {
  alarm_name          = "${local.name_prefix}-vaultdb-connection-errors"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "VaultDBConnectionErrors"
  namespace           = "Atlantis/VaultDB"
  period              = 300
  statistic           = "Sum"
  threshold           = 5
  alarm_description   = "High number of VaultDB connection errors"
  alarm_actions       = [aws_sns_topic.atlantis_alerts.arn]
  treat_missing_data  = "notBreaching"

  tags = local.common_tags
}

# Enhanced Deployment Monitoring
resource "aws_cloudwatch_log_metric_filter" "deployment_events" {
  name           = "${local.name_prefix}-deployment-events"
  log_group_name = aws_cloudwatch_log_group.atlantis.name
  pattern        = "[timestamp, request_id, level, msg=\"*deployment*\" || msg=\"*task*started*\" || msg=\"*task*stopped*\"]"

  metric_transformation {
    name      = "DeploymentEvents"
    namespace = "Atlantis/Deployment"
    value     = "1"
  }
}

resource "aws_cloudwatch_metric_alarm" "rolling_deployment_duration" {
  alarm_name          = "${local.name_prefix}-long-deployment-duration"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "ServiceDeploymentTime"
  namespace           = "AWS/ECS"
  period              = 900 # 15 minutes
  statistic           = "Maximum"
  threshold           = 900 # 15 minutes max for VaultDB rolling deployments
  alarm_description   = "Deployment taking longer than expected (VaultDB constraint)"
  alarm_actions       = [aws_sns_topic.atlantis_alerts.arn]
  treat_missing_data  = "notBreaching"

  dimensions = {
    ServiceName = aws_ecs_service.atlantis.name
    ClusterName = aws_ecs_cluster.main.name
  }

  tags = local.common_tags
}

# CI/CD Pipeline Monitoring
resource "aws_cloudwatch_log_metric_filter" "cicd_failures" {
  name           = "${local.name_prefix}-cicd-failures"
  log_group_name = aws_cloudwatch_log_group.atlantis.name
  pattern        = "[timestamp, request_id, level=\"ERROR\", msg=\"*CI/CD*\" || msg=\"*pipeline*failed*\" || msg=\"*deployment*failed*\"]"

  metric_transformation {
    name      = "CICDFailures"
    namespace = "Atlantis/CICD"
    value     = "1"
  }
}

resource "aws_cloudwatch_metric_alarm" "cicd_failure_rate" {
  alarm_name          = "${local.name_prefix}-cicd-failure-rate"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "CICDFailures"
  namespace           = "Atlantis/CICD"
  period              = 3600 # 1 hour
  statistic           = "Sum"
  threshold           = 3
  alarm_description   = "High CI/CD failure rate detected"
  alarm_actions       = [aws_sns_topic.atlantis_alerts.arn]
  treat_missing_data  = "notBreaching"

  tags = local.common_tags
}

# Atlantis-specific Performance Monitoring
resource "aws_cloudwatch_metric_alarm" "atlantis_plan_failures" {
  alarm_name          = "${local.name_prefix}-plan-failures"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "HTTPCode_Target_5XX_Count"
  namespace           = "AWS/ApplicationELB"
  period              = 300
  statistic           = "Sum"
  threshold           = 5
  alarm_description   = "High number of Atlantis plan/apply failures"
  alarm_actions       = [aws_sns_topic.atlantis_alerts.arn]
  treat_missing_data  = "notBreaching"

  dimensions = {
    LoadBalancer = aws_lb.atlantis.arn_suffix
  }

  tags = local.common_tags
}

# GitHub API Rate Limiting Monitoring
resource "aws_cloudwatch_log_metric_filter" "github_api_errors" {
  name           = "${local.name_prefix}-github-api-errors"
  log_group_name = aws_cloudwatch_log_group.atlantis.name
  pattern        = "[timestamp, request_id, level=\"ERROR\", msg=\"*github*\" || msg=\"*403*\" || msg=\"*rate*limit*\"]"

  metric_transformation {
    name      = "GitHubAPIErrors"
    namespace = "Atlantis/GitHub"
    value     = "1"
  }
}

resource "aws_cloudwatch_metric_alarm" "github_api_rate_limit" {
  alarm_name          = "${local.name_prefix}-github-api-rate-limit"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "GitHubAPIErrors"
  namespace           = "Atlantis/GitHub"
  period              = 300
  statistic           = "Sum"
  threshold           = 10
  alarm_description   = "GitHub API rate limiting or errors detected"
  alarm_actions       = [aws_sns_topic.atlantis_alerts.arn]
  treat_missing_data  = "notBreaching"

  tags = local.common_tags
}
